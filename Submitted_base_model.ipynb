{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install wandb\n",
        "# !pip install lightning"
      ],
      "metadata": {
        "id": "4KRnpUy22K5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MTL Base Model approach\n",
        "- Fine-tune mutli language DistilBert Model\n",
        "- Two sperate classification layer\n",
        "  - One output neuron for Soft label and soft label\n",
        "- Adam optimizer with cosine LR decay\n",
        "- Shuffle all datasets while training\n",
        "- Should be used as shared base for DataAsTask models\n",
        "\n",
        "#### Heads\n",
        "- Single neuron output for two layers\n",
        "  - BCE Loss for hard label and percentage of Soft Label\n",
        "  - Example: 2 of 6 annotator labeled 1:\n",
        "    - Hard label 0: Soft_label_1: 0.33 \n",
        "  - $Loss = (BCELoss(HardLabel) + BCELoss(SoftLabel1) * 2 )/2$\n",
        "- Only one Single layer &rarr; Focus on transformer fine tuning\n",
        "\n",
        "#### Possible Improvements\n",
        "- Romanian dataset improved results for this pre trained model\n",
        "  - It was not used for the submission run\n",
        "- Use another model than DistilBert\n",
        "- Different heads worked fine as well\n",
        "  - Just for Soft Labels: 2 heads BCE and KL Div on Soft labels\n",
        "- Weight the Error to compensate unbalanced datasets\n",
        "  - Did not improve this model (weights per batch: see commented code below)\n",
        "- Improved fine tuning of \"typical parameters\": LR, Optimizer, ...\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YmVysd855i69"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ylv-2VGIxW_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8iiDkW4Dm0UF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64326ed7-b0ad-4b52-ef99-98bff0621faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLUrOiEf2Sp6",
        "outputId": "bddce7b3-9060-45c8-e7cf-1728a492b659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msheuschk\u001b[0m (\u001b[33mcapture_disagreement\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from drive.MyDrive.cicl_data.helpers import read_data\n",
        "# from drive.MyDrive.cicl_data.code import CustomLabelDataset"
      ],
      "metadata": {
        "id": "ZedtcbT22LoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
        "# from datasets import Dataset\n",
        "import torch.nn.functional as Fun\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "from tqdm.notebook import tqdm\n"
      ],
      "metadata": {
        "id": "RssAgMX314if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 14\n",
        "torch.manual_seed(seed)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "uLkclSt57CSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = read_data()\n",
        "df_all = pd.concat([data_dict[k] for k in data_dict.keys()])"
      ],
      "metadata": {
        "id": "b0lq4E472D6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_romanian(usage=\"train\"):\n",
        "  \"\"\"@usage: 'train'/'dev'\"\"\"\n",
        "  current_file = '/content/drive/MyDrive/cicl_data/romanian/rom_'+ usage +'.json' \n",
        "  data = json.load(open(current_file, 'r', encoding = 'UTF-8'))                                   \n",
        " \n",
        "  def extract_soft_labels(row):\n",
        "    return list(row.values())\n",
        "\n",
        "  def transform_data(data, name):\n",
        "    df = pd.DataFrame(data).transpose()\n",
        "    df = df.astype({\"hard_label\": int}, errors='raise') \n",
        "    df['data_set'] = name\n",
        "    df[\"soft_list\"] = df[\"soft_label\"].apply(extract_soft_labels)\n",
        "    return df\n",
        "\n",
        "  df = transform_data(data, \"rom\")\n",
        "  return df"
      ],
      "metadata": {
        "id": "FK5BnTJtMkjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t8S3ZnDmNvui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rom_data = read_romanian()\n",
        "# rom_data[\"split\"] = \"train\"\n",
        "\n",
        "# rom_data_small = rom_data.sample(5000, random_state=42)\n",
        "# df_all = pd.concat([df_all, rom_data_small])\n",
        "\n",
        "# All data\n",
        "# df_all = pd.concat([df_all, rom_data])"
      ],
      "metadata": {
        "id": "04-u3N1fPR6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VQwO72nKPki-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_soft_labels(row):\n",
        "  return row[1]"
      ],
      "metadata": {
        "id": "SWiX09DWgdTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all[\"sl_1s\"] = df_all[\"soft_list\"].apply(extract_soft_labels)"
      ],
      "metadata": {
        "id": "OC2b9Z_QgPGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hSoiW5fghOLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretrained model"
      ],
      "metadata": {
        "id": "-FFCNddg5uOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertModel, DistilBertTokenizer"
      ],
      "metadata": {
        "id": "LBs7QjkT89dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Maybe load from wandb in future\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')"
      ],
      "metadata": {
        "id": "3K8_di7C2edU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import BertModel\n",
        "# base_model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\n",
        "# base_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "# class_model = AutoModelForSequenceClassification.from_pretrained(\"lanwuwei/GigaBERT-v4-Arabic-and-English\", num_labels=2)\n",
        "\n",
        "# output of model: https://huggingface.co/docs/transformers/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
      ],
      "metadata": {
        "id": "9G0OzXTNamcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MTLModel(nn.Module):\n",
        "  def __init__(self, base_model):\n",
        "    super().__init__()\n",
        "    self.bert = base_model\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.act = nn.Tanh()\n",
        "\n",
        "    # Soft head\n",
        "    self.lin_s1 = nn.Linear(768, 1)\n",
        "    self.lin_h1 = nn.Linear(768, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    \"\"\"a linear layer on top of the pooled output (https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#bertforsequenceclassification)\"\"\"\n",
        "\n",
        "    x = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    hidden_state = x[0]\n",
        "    pooler = hidden_state[:, 0]\n",
        "\n",
        "    x_s = self.lin_s1(pooler)\n",
        "    x_s = torch.flatten(self.sigmoid(x_s))\n",
        "\n",
        "    x_h = self.lin_h1(pooler)\n",
        "    x_h = torch.flatten(self.sigmoid(x_h))\n",
        "\n",
        "    return x_h.to(torch.float64), x_s.to(torch.float64)"
      ],
      "metadata": {
        "id": "NfZHklM7Z5Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\n",
        "model = MTLModel(base_model).to(device)"
      ],
      "metadata": {
        "id": "U6mefA7r6-Jx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2f30b5d-c25f-467a-d1b4-3a5ed285c8fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2FAzjgv351F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QUHaArOeRVhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data"
      ],
      "metadata": {
        "id": "OFp2k-qB51d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MaxLen = 240"
      ],
      "metadata": {
        "id": "qEMKxviVNIm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLabelDataset(Dataset):\n",
        "    def __init__(self, df_all):\n",
        "        self.text = list(map(self.tokenize_func, df_all[\"text\"]))\n",
        "        self.soft_labels = df_all[\"soft_list\"] \n",
        "        self.hard_labels = df_all[\"hard_label\"]\n",
        "        self.hard_labels_1h = Fun.one_hot(torch.tensor(df_all['hard_label'].values))\n",
        "        self.soft_labels_1s = df_all[\"sl_1s\"] # 0.33 of soft labels like {\"1\": 0.33, \"0\": 0.67}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "      \n",
        "    def tokenize_func(self, text):\n",
        "        return tokenizer(text, truncation=True, max_length=MaxLen, padding=\"max_length\", add_special_tokens=True)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input = {\"attention_mask\": torch.tensor(self.text[idx][\"attention_mask\"]),\n",
        "                 \"input_ids\": torch.tensor(self.text[idx][\"input_ids\"])}\n",
        "        return input, self.hard_labels_1h[idx], torch.tensor(self.soft_labels[idx]), torch.tensor(self.hard_labels[idx]), torch.tensor(self.soft_labels_1s[idx])\n"
      ],
      "metadata": {
        "id": "rmYCQ5IzGQSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Init dataset\n",
        "dataset = CustomLabelDataset(df_all)\n",
        "batch_size = 64\n",
        "\n",
        "train_size = len(dataset)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True)\n"
      ],
      "metadata": {
        "id": "Asl4kclO2egC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize Evaluation dataset\n",
        "data_dict_dev = read_data(\"dev\")\n",
        "df_dev = pd.concat([data_dict_dev[k] for k in data_dict_dev.keys()])\n",
        "\n",
        "df_dev[\"sl_1s\"] = df_dev[\"soft_list\"].apply(extract_soft_labels)\n",
        "\n",
        "dev_dataset = CustomLabelDataset(df_dev)\n",
        "dev_batch_size = 64\n",
        "dev_size = len(dev_dataset)\n",
        "\n",
        "dev_dataloader = DataLoader(\n",
        "    dev_dataset,\n",
        "    batch_size=dev_batch_size)"
      ],
      "metadata": {
        "id": "7rQ8gtQcbBXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(dataset[0][0][\"input_ids\"])"
      ],
      "metadata": {
        "id": "_kOz2_LHMJRo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "66b4bc6b-838f-4212-cf3b-fdac8ce2cbbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] النسويه يعني نصير رجل قولتك وبعدين اذا الوحده تبا تسرح وتمرح لازم تكون رجال [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VLFaz2RGDNp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization"
      ],
      "metadata": {
        "id": "NwEKXBtI59g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam, AdamW\n",
        "from transformers import get_cosine_schedule_with_warmup"
      ],
      "metadata": {
        "id": "4qtOLn70DRuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative\n",
        "num_epochs = 7\n",
        "total_steps = num_epochs * len(train_dataloader)\n",
        "warmup_steps = 0.1 * total_steps\n",
        "training_steps = 0.9 * total_steps\n",
        "\n",
        "LR = 5e-05\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr = LR)\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, training_steps)\n"
      ],
      "metadata": {
        "id": "gatHKq9qWYuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bce_loss = nn.BCEWithLogitsLoss()\n",
        "bce_loss = nn.BCELoss()\n",
        "# kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "# ce_loss = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "44oJzJhg2elN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" # Creates weights for BCE Loss (unused here)\n",
        "def create_batch_weigts_bce(labels):\n",
        "  n_batch_size = len(labels)\n",
        "  _, counts = torch.unique(labels, return_counts=True)\n",
        "  weight0 = torch.full((n_batch_size,), n_batch_size/ counts[0])\n",
        "  weight1 = torch.full((n_batch_size,), n_batch_size/ counts[1])\n",
        "  w = torch.where(labels==1, weight1, labels)\n",
        "  w = torch.where(w==0, weight0, w)\n",
        "  return w / 2\"\"\""
      ],
      "metadata": {
        "id": "ZiKLdCsNm4z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "ZHvCCIut6Ms8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init(\n",
        "    project=\"MTL_DBert\",\n",
        "    config={\n",
        "        \"epochs\": num_epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"device\": device,\n",
        "        \"Seed\": seed,\n",
        "        \"token_max_len\": MaxLen,\n",
        "        \"LR\": LR\n",
        "        },\n",
        "      save_code = True,\n",
        "      tags = [\"distilbert\", \"cosine_schedule\" \"MAxLen240\", \"MTL\", \"BCE\", \"Soft\", \"2H1N\" \"all_rom\"]\n",
        "      )"
      ],
      "metadata": {
        "id": "K6OcK6ZkRAbC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26d920ac-3959-4944-cf93-409498a92c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230128_144823-7siud5cc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/capture_disagreement/MTL_DBert/runs/7siud5cc\" target=\"_blank\">glowing-ox-21</a></strong> to <a href=\"https://wandb.ai/capture_disagreement/MTL_DBert\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href=\"https://wandb.ai/capture_disagreement/MTL_DBert\" target=\"_blank\">https://wandb.ai/capture_disagreement/MTL_DBert</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href=\"https://wandb.ai/capture_disagreement/MTL_DBert/runs/7siud5cc\" target=\"_blank\">https://wandb.ai/capture_disagreement/MTL_DBert/runs/7siud5cc</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from drive.MyDrive.cicl_data.helpers import ce_eval_func\n",
        "def ce_eval_func(model, eval_dataloader, eval_size, epsilon=1e-12, device=\"cuda\"):\n",
        "  model.eval()\n",
        "  cross_error = 0\n",
        "\n",
        "  for i, batch in enumerate(tqdm(eval_dataloader, 0)):\n",
        "    input_ids = batch[0][\"input_ids\"].to(device, dtype=torch.long)\n",
        "    attention_mask = batch[0][\"attention_mask\"].to(device, dtype=torch.long)\n",
        "    soft_labels = batch[2].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      _, pred = model(input_ids, attention_mask=attention_mask)\n",
        "    # pred = nn.Sigmoid()(pred)\n",
        "    pred = pred.reshape(len(pred), 1)\n",
        "    probabilities = torch.cat((1-pred, pred), dim=-1)\n",
        "    # probabilities = torch.softmax(pred, axis=-1)\n",
        "    \n",
        "    predictions = torch.clip(probabilities, epsilon, 1. - epsilon)\n",
        "    cross_error += -torch.sum(soft_labels * torch.log(predictions + 1e-9))\n",
        "\n",
        "  return cross_error / eval_size\n",
        "\n"
      ],
      "metadata": {
        "id": "4U6NEklMdfxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ce_before = ce_eval_func(model, dev_dataloader, dev_size, device=device)\n",
        "wandb.log({\"eval/ce_before_training\": ce_before})\n",
        "print(f\"CE before training: {ce_before}\")"
      ],
      "metadata": {
        "id": "S9IipQBcwOlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Ec7PBP6BHG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "last_ce = 10\n",
        "smallest_ce = 10\n",
        "eval_counter = False\n",
        "\n",
        "for e in range(num_epochs):\n",
        "  model.train()\n",
        "  loss_batches = 0\n",
        "  epoch_loss = 0\n",
        "  epoch_len = len(train_dataloader)\n",
        "\n",
        "  for i, batch in enumerate(train_dataloader):\n",
        "    input_ids = batch[0][\"input_ids\"].to(device, dtype=torch.long)\n",
        "    attention_mask = batch[0][\"attention_mask\"].to(device, dtype=torch.long)\n",
        "    soft_labels_1 = batch[4].to(device, dtype=torch.float64)\n",
        "    # soft_labels = batch[2].to(device, dtype=torch.float64)\n",
        "    # hard_labels = batch[1].to(device, dtype=torch.float64)\n",
        "    hard_label = batch[3].to(device, dtype=torch.float64)\n",
        "\n",
        "    # predict\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    pred_hl, pred_sl = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # Loss\n",
        "    loss = bce_loss(pred_sl, soft_labels_1) * 2\n",
        "    loss += bce_loss(pred_hl, hard_label)\n",
        "    loss /= 2\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # Log\n",
        "    loss_batches += loss.item()\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    log_n_batches = 20\n",
        "    if i % log_n_batches == 0:\n",
        "      if i != 0:\n",
        "        print(f\"{e+1}: Last {log_n_batches} batches avg loss: {loss_batches/log_n_batches:>7f}  [{i}/{epoch_len}]\")\n",
        "        wandb.log({\"train/loss_over_batches\": loss_batches/log_n_batches})\n",
        "        wandb.log({\"train/epochs\": e})\n",
        "      loss_batches = 0\n",
        "  \n",
        "  epoch_loss /= i  \n",
        "  print(f\"Epoch [{e+1}/{num_epochs}] mean loss: {epoch_loss:>6f}\")\n",
        "  wandb.log({\"train/epoch_loss\": epoch_loss})\n",
        "\n",
        "  # Eval error\n",
        "  ce = ce_eval_func(model, dev_dataloader, dev_size, device=device)\n",
        "  print(f\"Epoch [{e+1}/{num_epochs}] Eval CE  : {ce:>6f}\")\n",
        "  wandb.log({\"eval/epoch_ce\": ce})\n",
        "\n",
        "  # Stop after Eval CE raises 2 times in a row (Simple early stopping)\n",
        "  if ce > last_ce:\n",
        "    if eval_counter is True:\n",
        "      print(\"Interrupt: Eval Error is raising\")\n",
        "      break;\n",
        "    eval_counter = True\n",
        "  elif ce < smallest_ce:\n",
        "    torch.save(model.state_dict(), 'model.pt')\n",
        "    print(f\"Epoch [{e+1}/{num_epochs}] Save model state\")\n",
        "    eval_counter = False\n",
        "    smallest_ce = ce\n",
        "  else:\n",
        "    eval_counter = False\n",
        "  \n",
        "  last_ce = ce\n",
        "\n"
      ],
      "metadata": {
        "id": "msKUOWi24GoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model dependent improvements:"
      ],
      "metadata": {
        "id": "lZ5Ws3sQXfaO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ICsFyZLz4UX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "QB3Oa4j36PJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_best = MTLModel(base_model)\n",
        "model_best.load_state_dict(torch.load('model.pt'))\n",
        "model_best = model_best.to(device)"
      ],
      "metadata": {
        "id": "iwGwtu0B6BbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Cross Entropy Error\n",
        "cross_error = ce_eval_func(model_best, dev_dataloader, dev_size, device=device)\n",
        "print(f\"CE error: {cross_error}\")\n",
        "wandb.log({\"dev/ce\": cross_error})"
      ],
      "metadata": {
        "id": "dcCmaLsH6iOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "cross_error = 0\n",
        "epsilon = 1e-12\n",
        "for i, batch in enumerate(dev_dataloader):\n",
        "  input_ids = batch[0][\"input_ids\"].to(device, dtype = torch.long)\n",
        "  attention_mask = batch[0][\"attention_mask\"].to(device, dtype = torch.long)\n",
        "  soft_labels = batch[2].to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    pred = model(input_ids, attention_mask=attention_mask)\n",
        "  # pred = pred.reshape(len(pred), 1)\n",
        "  # probabilities = torch.cat((1-pred, pred), dim=-1)\n",
        "  probabilities = torch.softmax(pred, axis=-1)\n",
        "  predictions = torch.clip(probabilities, epsilon, 1. - epsilon)\n",
        "  cross_error += -torch.sum(soft_labels * torch.log(predictions + 1e-9))\n",
        "  break\n",
        "\n"
      ],
      "metadata": {
        "id": "suoX3FT2jBBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions)\n",
        "print(soft_labels)"
      ],
      "metadata": {
        "id": "M92VuoY1jBEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dPMlfy-U7OBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finish"
      ],
      "metadata": {
        "id": "eydOBaKsiz5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raise Exception"
      ],
      "metadata": {
        "id": "_1HCFnnArZ9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.state_dict(), 'model.pt')\n",
        "# model.load_state_dict(torch.load(PATH), strict=False)\n",
        "artifact = wandb.Artifact(name='model_param', type='model')\n",
        "artifact.add_file(local_path=\"model.pt\")\n",
        "run.log_artifact(artifact);"
      ],
      "metadata": {
        "id": "Yek9Cza6lOUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "U36WeQhk43v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3vOTDCRf43yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bYN4cn0tgwCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZNTuvRmYgwJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TSV files"
      ],
      "metadata": {
        "id": "fqrKNQOPgnmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raise Exception"
      ],
      "metadata": {
        "id": "j1jOsEYhxRc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv"
      ],
      "metadata": {
        "id": "hv13ro9PgmWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepaths = [\"/content/ArMIS_results.tsv\", \"/content/ConvAbuse_results.tsv\", \"/content/HS-Brexit_results.tsv\", \"/content/MD-Agreement_results.tsv\"]\n",
        "epsilon = 1e-12\n",
        "\n",
        "for fp in filepaths:\n",
        "  if os.path.exists(fp):\n",
        "    os.remove(fp)\n",
        "\n",
        "for key in data_dict_dev.keys():\n",
        "  data_dict_dev[key][\"sl_1s\"] = data_dict_dev[key][\"soft_list\"].apply(extract_soft_labels)\n",
        "  tsv_dataset = CustomLabelDataset(data_dict_dev[key])\n",
        "  tsv_dataloader = DataLoader(tsv_dataset, shuffle=False, batch_size=1)\n",
        "  filepath_write = f\"/content/{key}_results.tsv\"\n",
        "  \n",
        "  if \"HS-\" in key:\n",
        "    task = \"HS\"\n",
        "  elif \"MD-\" in key:\n",
        "    task = \"MD\"\n",
        "  elif \"Conv\" in key:\n",
        "    task = \"Abu\"\n",
        "  elif \"MIS\" in key:\n",
        "    task = \"Mis\"\n",
        "\n",
        "  with open(filepath_write, 'w', newline='') as tsvfile:\n",
        "      writer = csv.writer(tsvfile, delimiter='\\t', lineterminator='\\n')\n",
        "      for i, batch in enumerate(tqdm(tsv_dataloader, 0)):\n",
        "        input_ids = batch[0][\"input_ids\"].to(device, dtype = torch.long)\n",
        "        attention_mask = batch[0][\"attention_mask\"].to(device, dtype = torch.long)\n",
        "        token_type_ids = batch[0][\"token_type_ids\"].to(device, dtype = torch.long)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          pred = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, task=task)\n",
        "        # logits = pred.logits\n",
        "        # probability = torch.softmax(pred, axis=-1)\n",
        "        pred = pred.reshape(len(pred), 1)\n",
        "        probability = torch.cat((1-pred, pred), dim=-1)\n",
        "        # probability = torch.softmax(pred, axis=-1)\n",
        "        prediction = torch.round(pred)\n",
        "        probability = torch.clip(probability, epsilon, 1. - epsilon) # Really necessary?\n",
        "        writer.writerow([int(prediction[0].item()), probability[0][0].item(), probability[0][1].item()])\n"
      ],
      "metadata": {
        "id": "6YA4In8ggmZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2QzZbIRBVCkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "filepath = \"res.zip\" \n",
        "\n",
        "if os.path.exists(filepath):\n",
        "    os.remove(filepath)\n",
        "\n",
        "#loop over filepath names throws an string index out of range for whatever reason(also can't use content here, not sure why)\n",
        "with ZipFile(filepath, 'w') as zipObj:\n",
        "  zipObj.write(\"MD-Agreement_results.tsv\")\n",
        "  zipObj.write(\"ArMIS_results.tsv\")\n",
        "  zipObj.write(\"HS-Brexit_results.tsv\")\n",
        "  zipObj.write(\"ConvAbuse_results.tsv\")"
      ],
      "metadata": {
        "id": "Y5E4llTxgmcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"res.zip\")"
      ],
      "metadata": {
        "id": "YvIfIi4Tgmga"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}