@inproceedings{lan2020gigabert,
  author     = {Lan, Wuwei and Chen, Yang and Xu, Wei and Ritter, Alan},
    title      = {GigaBERT: Zero-shot Transfer Learning from English to Arabic},
    booktitle  = {Proceedings of The 2020 Conference on Empirical Methods on Natural Language Processing (EMNLP)},
    year       = {2020}
  } 
@article{DBLP:journals/corr/abs-1912-10985,
  author    = {Felix Dangel and
               Frederik Kunstner and
               Philipp Hennig},
  title     = {BackPACK: Packing more into backprop},
  journal   = {CoRR},
  volume    = {abs/1912.10985},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.10985},
  eprinttype = {arXiv},
  eprint    = {1912.10985},
  timestamp = {Fri, 03 Jan 2020 16:10:45 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-10985.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{uma-etal-2021-semeval,
    title = "{S}em{E}val-2021 Task 12: Learning with Disagreements",
    author = "Uma, Alexandra  and
      Fornaciari, Tommaso  and
      Dumitrache, Anca  and
      Miller, Tristan  and
      Chamberlain, Jon  and
      Plank, Barbara  and
      Simpson, Edwin  and
      Poesio, Massimo",
    booktitle = "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.semeval-1.41",
    doi = "10.18653/v1/2021.semeval-1.41",
    pages = "338--347",
    abstract = "Disagreement between coders is ubiquitous in virtually all datasets annotated with human judgements in both natural language processing and computer vision. However, most supervised machine learning methods assume that a single preferred interpretation exists for each item, which is at best an idealization. The aim of the SemEval-2021 shared task on learning with disagreements (Le-Wi-Di) was to provide a unified testing framework for methods for learning from data containing multiple and possibly contradictory annotations covering the best-known datasets containing information about disagreements for interpreting language and classifying images. In this paper we describe the shared task and its results.",
}
@INPROCEEDINGS{9010969,

  author={Peterson, Joshua and Battleday, Ruairidh and Griffiths, Thomas and Russakovsky, Olga},

  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 

  title={Human Uncertainty Makes Classification More Robust}, 

  year={2019},

  volume={},

  number={},

  pages={9616-9625},

  doi={10.1109/ICCV.2019.00971}}
@article{Uma_Fornaciari_Hovy_Paun_Plank_Poesio_2020, title={A Case for Soft Loss Functions}, volume={8}, url={https://ojs.aaai.org/index.php/HCOMP/article/view/7478}, DOI={10.1609/hcomp.v8i1.7478}, abstractNote={&lt;p class=&quot;abstract&quot;&gt;Recently, Peterson et al. provided evidence of the benefits of using probabilistic soft labels generated from crowd annotations for training a computer vision model, showing that using such labels maximizes performance of the models over unseen data. In this paper, we generalize these results by showing that training with soft labels is an effective method for using crowd annotations in several other ai tasks besides the one studied by Peterson &lt;em&gt;et al.&lt;/em&gt;, and also when their performance is compared with that of state-of-the-art methods for learning from crowdsourced data.&lt;/p&gt;}, number={1}, journal={Proceedings of the AAAI Conference on Human Computation and Crowdsourcing}, author={Uma, Alexandra and Fornaciari, Tommaso and Hovy, Dirk and Paun, Silviu and Plank, Barbara and Poesio, Massimo}, year={2020}, month={Oct.}, pages={173-177} }
@inproceedings{osei-brefo-etal-2021-uor,
    title = "{UOR} at {S}em{E}val-2021 Task 12: On Crowd Annotations; Learning with Disagreements to optimise crowd truth",
    author = "Osei-Brefo, Emmanuel  and
      Markchom, Thanet  and
      Liang, Huizhi",
    booktitle = "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.semeval-1.186",
    doi = "10.18653/v1/2021.semeval-1.186",
    pages = "1303--1309",
    abstract = "Crowdsourcing has been ubiquitously used for annotating enormous collections of data. However, the major obstacles to using crowd-sourced labels are noise and errors from non-expert annotations. In this work, two approaches dealing with the noise and errors in crowd-sourced labels are proposed. The first approach uses Sharpness-Aware Minimization (SAM), an optimization technique robust to noisy labels. The other approach leverages a neural network layer called softmax-Crowdlayer specifically designed to learn from crowd-sourced annotations. According to the results, the proposed approaches can improve the performance of the Wide Residual Network model and Multi-layer Perception model applied on crowd-sourced datasets in the image processing domain. It also has similar and comparable results with the majority voting technique when applied to the sequential data domain whereby the Bidirectional Encoder Representations from Transformers (BERT) is used as the base model in both instances.",
}
@inproceedings{basile-etal-2021-need,
    title = "We Need to Consider Disagreement in Evaluation",
    author = "Basile, Valerio  and
      Fell, Michael  and
      Fornaciari, Tommaso  and
      Hovy, Dirk  and
      Paun, Silviu  and
      Plank, Barbara  and
      Poesio, Massimo  and
      Uma, Alexandra",
    booktitle = "Proceedings of the 1st Workshop on Benchmarking: Past, Present and Future",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.bppf-1.3",
    doi = "10.18653/v1/2021.bppf-1.3",
    pages = "15--21",
    abstract = "Evaluation is of paramount importance in data-driven research fields such as Natural Language Processing (NLP) and Computer Vision (CV). Current evaluation practice largely hinges on the existence of a single {``}ground truth{''} against which we can meaningfully compare the prediction of a model. However, this comparison is flawed for two reasons. 1) In many cases, more than one answer is correct. 2) Even where there is a single answer, disagreement among annotators is ubiquitous, making it difficult to decide on a gold standard. We argue that the current methods of adjudication, agreement, and evaluation need serious reconsideration. Some researchers now propose to minimize disagreement and to fix datasets. We argue that this is a gross oversimplification, and likely to conceal the underlying complexity. Instead, we suggest that we need to better capture the sources of disagreement to improve today{'}s evaluation practice. We discuss three sources of disagreement: from the annotator, the data, and the context, and show how this affects even seemingly objective tasks. Datasets with multiple annotations are becoming more common, as are methods to integrate disagreement into modeling. The logical next step is to extend this to evaluation.",
}
@article{Rodrigues_Pereira_2018, title={Deep Learning from Crowds}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11506}, DOI={10.1609/aaai.v32i1.11506}, abstractNote={ &lt;p&gt; Over the last few years, deep learning has revolutionized the field of machine learning by dramatically improving the state-of-the-art in various domains. However, as the size of supervised artificial neural networks grows, typically so does the need for larger labeled datasets. Recently, crowdsourcing has established itself as an efficient and cost-effective solution for labeling large sets of data in a scalable manner, but it often requires aggregating labels from multiple noisy contributors with different levels of expertise. In this paper, we address the problem of learning deep neural networks from crowds. We begin by describing an EM algorithm for jointly learning the parameters of the network and the reliabilities of the annotators. Then, a novel general-purpose crowd layer is proposed, which allows us to train deep neural networks end-to-end, directly from the noisy labels of multiple annotators, using only backpropagation. We empirically show that the proposed approach is able to internally capture the reliability and biases of different annotators and achieve new state-of-the-art results for various crowdsourced datasets across different settings, namely classification, regression and sequence labeling. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Rodrigues, Filipe and Pereira, Francisco}, year={2018}, month={Apr.} }


@article{DBLP:journals/corr/abs-1912-01987,
  author    = {Edwin Simpson and
               Iryna Gurevych},
  title     = {Scalable Bayesian Preference Learning for Crowds},
  journal   = {CoRR},
  volume    = {abs/1912.01987},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.01987},
  eprinttype = {arXiv},
  eprint    = {1912.01987},
  timestamp = {Thu, 02 Jan 2020 18:08:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-01987.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{4664785,  author={Zhou, Zhi-Hua},  booktitle={2008 IEEE International Conference on Granular Computing},   title={Semi-supervised learning by disagreement},   year={2008},  volume={},  number={},  pages={93-93},  doi={10.1109/GRC.2008.4664785}}
@article{10.1613/jair.1.12752,
author = {Uma, Alexandra N. and Fornaciari, Tommaso and Hovy, Dirk and Paun, Silviu and Plank, Barbara and Poesio, Massimo},
title = {Learning from Disagreement: A Survey},
year = {2022},
issue_date = {Jan 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {72},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12752},
doi = {10.1613/jair.1.12752},
abstract = {Many tasks in Natural Language Processing (NLP) and Computer Vision (CV) offer evidence that humans disagree, from objective tasks such as part-of-speech tagging to more subjective tasks such as classifying an image or deciding whether a proposition follows from certain premises. While most learning in artificial intelligence (AI) still relies on the assumption that a single (gold) interpretation exists for each item, a growing body of research aims to develop learning methods that do not rely on this assumption. In this survey, we review the evidence for disagreements on NLP and CV tasks, focusing on tasks for which substantial datasets containing this information have been created. We discuss the most popular approaches to training models from datasets containing multiple judgments potentially in disagreement. We systematically compare these different approaches by training them with each of the available datasets, considering several ways to evaluate the resulting models. Finally, we discuss the results in depth, focusing on four key research questions, and assess how the type of evaluation and the characteristics of a dataset determine the answers to these questions. Our results suggest, first of all, that even if we abandon the assumption of a gold standard, it is still essential to reach a consensus on how to evaluate models. This is because the relative performance of the various training methods is critically affected by the chosen form of evaluation. Secondly, we observed a strong dataset effect. With substantial datasets, providing many judgments by high-quality coders for each item, training directly with soft labels achieved better results than training from aggregated or even gold labels. This result holds for both hard and soft evaluation. But when the above conditions do not hold, leveraging both gold and soft labels generally achieved the best results in the hard evaluation. All datasets and models employed in this paper are freely available as supplementary materials.},
journal = {J. Artif. Int. Res.},
month = {jan},
pages = {1385–1470},
numpages = {86},
keywords = {vision, natural language, machine learning, uncertainty}
}